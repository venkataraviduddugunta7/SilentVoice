# ğŸš€ SilentVoice ML Training Guide

## ğŸ“‹ **Project Overview**

SilentVoice is a real-time sign language translator with:
- **Frontend**: React/Next.js web app with 3D avatar
- **Backend**: FastAPI server with TensorFlow ML model
- **ML Pipeline**: Hand tracking â†’ LSTM model â†’ Gesture recognition

---

## ğŸ—‚ï¸ **Project Structure**

```
silent-voice/
â”œâ”€â”€ frontend/                    # React/Next.js Web App
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/page.tsx        # Main UI (camera, avatar, controls)
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ CameraView.tsx  # MediaPipe hand tracking
â”‚   â”‚   â”‚   â”œâ”€â”€ GLBViewer.tsx   # 3D avatar with animations
â”‚   â”‚   â”‚   â””â”€â”€ OutputText.tsx  # Text display
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useRecognizer.ts # WebSocket connection
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ websocket.ts    # Real-time communication
â”‚   â””â”€â”€ public/models/          # 3D avatar files
â”‚
â”œâ”€â”€ backend/                     # Python FastAPI Server
â”‚   â”œâ”€â”€ main.py                 # Server startup
â”‚   â”œâ”€â”€ api.py                  # WebSocket API & gesture processing
â”‚   â”œâ”€â”€ model.py                # TensorFlow LSTM model architecture
â”‚   â”œâ”€â”€ train_model.py          # Training script
â”‚   â”œâ”€â”€ collect_training_data.py # Data collection tool
â”‚   â”œâ”€â”€ model_service.py        # Model loading service
â”‚   â””â”€â”€ requirements.txt        # Python dependencies
â”‚
â””â”€â”€ Training Files (You'll create)
    â”œâ”€â”€ training_data/          # Gesture recordings
    â”‚   â”œâ”€â”€ HELLO/             # Hello gesture samples
    â”‚   â”œâ”€â”€ THANKS/            # Thanks gesture samples
    â”‚   â””â”€â”€ YES/               # Yes gesture samples
    â””â”€â”€ models/                # Trained models
        â””â”€â”€ sign_language_model.h5
```

---

## ğŸ¯ **How Machine Learning Works in SilentVoice**

### **1. Data Flow:**
```
Camera â†’ MediaPipe â†’ Hand Landmarks â†’ LSTM Model â†’ Gesture Prediction â†’ Avatar Animation
```

### **2. Current Status:**
- âœ… **Frontend**: Captures hand landmarks using MediaPipe
- âœ… **Backend**: Rule-based gesture detection (basic fallback)
- âš ï¸ **ML Model**: Architecture ready, needs training data
- âœ… **Avatar**: Responds to gesture predictions with animations

---

## ğŸ‹ï¸ **Training Your ML Model**

### **Step 1: Collect Training Data**

```bash
cd backend
source venv/bin/activate  # Activate virtual environment
python collect_training_data.py
```

**What this does:**
- Opens your webcam
- Records hand landmarks for each gesture
- Saves sequences as JSON files
- Organizes by gesture name

**Instructions:**
1. Select gesture name (e.g., "HELLO")
2. Press SPACE to start recording
3. Perform the gesture slowly and clearly
4. Press SPACE to stop recording
5. Repeat 20-30 times per gesture
6. Record multiple gestures (HELLO, THANKS, YES, NO, PEACE, etc.)

### **Step 2: Train the Model**

```bash
python train_model.py
```

**What this does:**
- Loads all your recorded gesture data
- Trains an LSTM neural network
- Saves the trained model as `models/sign_language_model.h5`
- Shows training progress and accuracy

### **Step 3: Test Your Model**

Restart your backend server - it will automatically load your trained model:

```bash
python main.py
```

---

## ğŸ® **Frontend Development Guide**

### **Key React Components:**

#### **1. Main App (`frontend/src/app/page.tsx`)**
- Controls UI state (recording, debug mode, language)
- Manages WebSocket connection
- Handles gesture predictions

#### **2. Camera Component (`frontend/src/components/CameraView.tsx`)**
- Uses MediaPipe for hand tracking
- Sends hand landmarks to backend
- Shows real-time hand detection

#### **3. Avatar Component (`frontend/src/components/GLBViewer.tsx`)**
- Renders 3D Ready Player Me avatar
- Animates gestures based on predictions
- Handles facial expressions

#### **4. WebSocket Hook (`frontend/src/hooks/useRecognizer.ts`)**
- Manages real-time communication
- Handles gesture predictions
- Manages connection status

### **Adding New Features:**

#### **Add New Gestures:**
1. **Collect Data**: Use `collect_training_data.py`
2. **Update Backend**: Add gesture to `backend/api.py`
3. **Add Animation**: Update `GLBViewer.tsx` gesture animations
4. **Retrain Model**: Run `train_model.py`

#### **Improve UI:**
- Edit `frontend/src/app/page.tsx` for layout
- Modify `frontend/src/app/globals.css` for styling
- Add new components in `frontend/src/components/`

---

## ğŸ”§ **Development Commands**

### **Backend:**
```bash
cd backend
source venv/bin/activate
python main.py                    # Start server
python collect_training_data.py   # Collect gesture data
python train_model.py            # Train ML model
```

### **Frontend:**
```bash
cd frontend
npm run dev                      # Start development server
npm run build                    # Build for production
```

---

## ğŸ“Š **Training Tips**

### **For Better Accuracy:**
1. **Record 30+ samples** per gesture
2. **Vary hand positions** (left/right, different angles)
3. **Record in different lighting** conditions
4. **Include negative samples** (random hand movements)
5. **Keep gestures consistent** but natural

### **Model Parameters** (in `model.py`):
- `sequence_length = 30`: Frames per gesture (adjust for longer/shorter gestures)
- `lstm_units = 128`: Model complexity (increase for more gestures)
- `dropout = 0.3`: Prevents overfitting (adjust if model memorizes training data)

---

## ğŸ¯ **Next Development Steps**

### **Immediate (This Week):**
1. âœ… Collect training data for 5-10 basic gestures
2. âœ… Train your first ML model
3. âœ… Test gesture recognition accuracy
4. âœ… Add more gesture animations to avatar

### **Short Term (Next 2 Weeks):**
1. ğŸ“ˆ Expand to 20+ gestures
2. ğŸ¨ Improve avatar animations
3. ğŸ“± Add mobile support
4. ğŸ”Š Add text-to-speech output

### **Long Term (Next Month):**
1. ğŸŒ Deploy to web hosting
2. ğŸ“š Add gesture learning mode
3. ğŸ‘¥ Multi-user support
4. ğŸ† Competition-ready features

---

## ğŸ†˜ **Troubleshooting**

### **Model Not Loading:**
- Check `models/sign_language_model.h5` exists
- Verify training completed successfully
- Check backend logs for errors

### **Poor Accuracy:**
- Collect more training data
- Ensure consistent gesture performance
- Adjust model parameters
- Add data augmentation

### **WebSocket Issues:**
- Ensure backend is running on port 8000
- Check firewall settings
- Verify frontend connects to correct URL

---

## ğŸ“ **Need Help?**

Your SilentVoice project is ready for ML training! Start with collecting data for basic gestures like HELLO, THANKS, YES, NO, and PEACE. The system will automatically use your trained model once it's ready.

**Ready to train your first model? Run:**
```bash
cd backend && python collect_training_data.py
```
